settings:
  #################   DATASET CONFIG   ###################

  trainloader:
    name: NumpyFeatureLoader
    args:
      root_dir: "/content/data/flickr30k_images/flickr30k_images"
      ann_path: "/content/data/train.json"
      feat_dir: "/content/data/bottom_up"
      text_dir: "/content/data/bert_features"
      batch_size: 128

  valloader:
    name: RawNumpyFeatureLoader
    args:
      root_dir: "/content/data/flickr30k_images/flickr30k_images"
      ann_path: "/content/data/val.json"
      feat_dir: "/content/data/bottom_up"
      text_dir: "/content/data/bert_features"
      batch_size: 128
      num_workers: 2

  ## Those dataset for metric evaluation
  valset1:
    name: BottomUpSet
    args:
      feat_dir: "/content/data/bottom_up"
      ann_path: "/content/data/val.json"
      
  valset2:
    name: BertSet
    args:
      feat_dir: "/content/data/bert_features"
      ann_path: "/content/data/val.json"

  #################   TRAINING CONFIG   ###################
  
  globals:
    gpu_devices: '0'                     # supports multi-gpus
    num_epochs: 100
    mixed_precision: False                # whether to use nvidia apex
    total_accumulate_steps: 0           # step * batch_size, not use if equal 0, gradient accumulation
  
  model:
    name: "TERN"
    args:
      d_embed: 1024                      # output_dim
      d_model: 768                       # model dim
      d_ff: 2048                         # feed-forward dim
      aggregation: "mean"               # [mean, first]
      N_v: 4                             # number of layers
      N_l: 0                             # number of layers
      heads: 4                           # number of heads
      dropout: 0.1
      precomp_bert: True                # whether to use bert feature extractor
      num_sw_layers: 0                       #number of shared-weight layer


  loss:
    name: "contrastive"                # [contrastive, nxtent, custom_nxtent, triplet]
    # distance: 
    #   name: "cosine"                   # [cosine, dot, euclidean]      
    # reducer:
    #   name: "threshold"
    #   high: 0.2
    # regularizer:                     # [l1, l2, mean]
    #   name: "l2"
      
  # learning rate policy
  lr_policy:
    name: "adam"                         #[adam|sgd]
    lr: 0.001                            #[adam: 1e-3 | sgd: 1e-2]
    momentum: 0.937
    weight_decay: 0.0005

  lr_scheduler:
    name: "cosine2"                      #[plateau | cosine | 1cycle-yolo | 1cycle]
                                        # if need to specify more scheduler arguments, do it here

